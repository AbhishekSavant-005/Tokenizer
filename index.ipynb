{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a7117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./corpus.txt\", encoding=\"utf-8\") as f:\n",
    "    dataset = f.read()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data= dataset.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b79d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nocap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>glowup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>wavy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>boujee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>glowedup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>zoomer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "0         lit\n",
       "1         bet\n",
       "2       nocap\n",
       "3         cap\n",
       "4        flex\n",
       "..        ...\n",
       "111    glowup\n",
       "112      wavy\n",
       "113    boujee\n",
       "114  glowedup\n",
       "115    zoomer\n",
       "\n",
       "[116 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "293482cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def get_pair_freq(vocab):\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        for i in range(len(word)-1):\n",
    "            pair = (word[i] , word[i+1])\n",
    "            pairs[pair] += freq\n",
    "        \n",
    "    return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc65baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, vocab):\n",
    "    new_vocab = {}\n",
    "    bigram = ' '.join(pair)\n",
    "    replacement = ''.join(pair)  # e.g. ('l','o') → 'lo'\n",
    "\n",
    "    for word, freq in vocab.items():\n",
    "        # Convert tuple of symbols to string with spaces between symbols\n",
    "        word_str = ' '.join(word)\n",
    "        # Replace all exact occurrences of the bigram\n",
    "        new_word_str = word_str.replace(bigram, replacement)\n",
    "        # Convert back to tuple for consistent representation\n",
    "        new_vocab[tuple(new_word_str.split(' '))] = freq\n",
    "\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbc42137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_bpe(data, num_merges):\n",
    "    vocab_dict = {}\n",
    "    for word in data:\n",
    "        char = list(word) + [\"</w>\"]\n",
    "        vocab_dict[tuple(char)]  = vocab_dict.get(tuple(char), 0) + 1\n",
    "    \n",
    "    merges = []\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_pair_freq(vocab_dict)\n",
    "        if not pairs:\n",
    "            break\n",
    "        best = max(pairs, key=pairs.get)\n",
    "        merges.append(best)\n",
    "\n",
    "        vocab_dict = merge_vocab(best, vocab_dict)\n",
    "    \n",
    "    return merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9863ddb",
   "metadata": {},
   "source": [
    "# tokenization (encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a4ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_in_word(word, pair):\n",
    "    \"\"\"Merge the pair inside a single word represented as a list of symbols.\"\"\"\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(word):\n",
    "        if i < len(word) - 1 and (word[i], word[i+1]) == pair:\n",
    "            merged.append(word[i] + word[i+1])\n",
    "            i += 2\n",
    "        else:\n",
    "            merged.append(word[i])\n",
    "            i += 1\n",
    "    return merged\n",
    "\n",
    "def encode_word(word, merges):\n",
    "    word = list(word) + [\"</w>\"]\n",
    "    while True:\n",
    "        pairs = [(word[i], word[i+1]) for i in range(len(word)-1)]\n",
    "        merge_candidates = [p for p in pairs if p in merges]\n",
    "        if not merge_candidates:\n",
    "            break\n",
    "        # Merge the first applicable one according to merges priority\n",
    "        pair_to_merge = min(merge_candidates, key=lambda p: merges.index(p))\n",
    "        word = merge_in_word(word, pair_to_merge)\n",
    "    return word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf74b50",
   "metadata": {},
   "source": [
    "# Assigning id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d21c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: ['d', 'e', 'l', 'u', 'l', 'u', '</w>']\n",
      "unique tokens: ['d', 'e', 'l', 'u', '</w>']\n",
      "ids: [0, 1, 2, 3, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Assign ids for an example encoded word (compute merges/encoded if missing)\n",
    "if 'merges' not in globals():\n",
    "    # token_bpe expects an iterable of words\n",
    "    merges = token_bpe(dataset.split(), num_merges=10)\n",
    "\n",
    "if 'encoded' not in globals():\n",
    "    encoded = encode_word(\"delulu\", merges)\n",
    "\n",
    "tokens = encoded  # encoded should come from encode_word(...)\n",
    "# preserve token order and remove duplicates when creating the vocab\n",
    "unique_tokens = list(dict.fromkeys(tokens))\n",
    "token_to_id = {token: i for i, token in enumerate(unique_tokens)}\n",
    "id_to_token = {i: token for token, i in token_to_id.items()}\n",
    "\n",
    "ids = [token_to_id[tok] for tok in tokens]\n",
    "\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"unique tokens:\", unique_tokens)\n",
    "print(\"ids:\", ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c151903",
   "metadata": {},
   "source": [
    "# Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35672bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids, id_to_token):\n",
    "    tokens = [id_to_token[i] for i in ids]\n",
    "    text = \"\".join(tokens).replace(\"</w>\", \" \")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c83439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned merges: [('\\n', '</w>'), ('e', '</w>'), ('a', '</w>'), ('i', '</w>'), ('t', '</w>'), ('o', '</w>'), ('s', '</w>'), ('c', '</w>'), ('n', '</w>'), ('g', '</w>')]\n",
      "Tokens: ['d', 'e', 'l', 'u', 'l', 'u', '</w>']\n"
     ]
    }
   ],
   "source": [
    "merges = token_bpe(dataset, num_merges=10)\n",
    "print(\"Learned merges:\", merges)\n",
    "\n",
    "encoded = encode_word(\"delulu\", merges)\n",
    "print(\"Tokens:\", encoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
